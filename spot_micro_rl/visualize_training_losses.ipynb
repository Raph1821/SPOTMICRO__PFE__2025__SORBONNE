{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecf9df0",
   "metadata": {},
   "source": [
    "# Visualisation des Losses - SAC & TD3\n",
    "\n",
    "Ce notebook permet d'analyser et comparer les losses pendant l'entraÃ®nement des algorithmes SAC et TD3.\n",
    "\n",
    "**MÃ©triques visualisÃ©es:**\n",
    "- Critic Loss (SAC & TD3)\n",
    "- Actor Loss (SAC & TD3)\n",
    "- Alpha Loss (SAC uniquement)\n",
    "- Alpha value (SAC - tempÃ©rature d'entropie)\n",
    "- RÃ©compenses (comparaison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothÃ¨ques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… BibliothÃ¨ques importÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7fa57",
   "metadata": {},
   "source": [
    "## 1. Chargement des DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13765c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins vers les logs\n",
    "logs_dir = Path('../results/training_logs')\n",
    "\n",
    "# Trouver les fichiers de logs les plus rÃ©cents\n",
    "sac_files = sorted(glob.glob(str(logs_dir / 'sac_training_*.csv')))\n",
    "td3_files = sorted(glob.glob(str(logs_dir / 'td3_training_*.csv')))\n",
    "\n",
    "print(f\"Fichiers SAC trouvÃ©s: {len(sac_files)}\")\n",
    "print(f\"Fichiers TD3 trouvÃ©s: {len(td3_files)}\")\n",
    "\n",
    "# Charger le dernier fichier de chaque algorithme\n",
    "if sac_files:\n",
    "    sac_log = sac_files[-1]\n",
    "    df_sac = pd.read_csv(sac_log)\n",
    "    print(f\"\\n SAC chargÃ©: {Path(sac_log).name}\")\n",
    "    print(f\"   Episodes: {len(df_sac)}\")\n",
    "    print(f\"   Colonnes: {list(df_sac.columns)}\")\n",
    "else:\n",
    "    print(\" Aucun log SAC trouvÃ©\")\n",
    "    df_sac = None\n",
    "\n",
    "if td3_files:\n",
    "    td3_log = td3_files[-1]\n",
    "    df_td3 = pd.read_csv(td3_log)\n",
    "    print(f\"\\nâœ… TD3 chargÃ©: {Path(td3_log).name}\")\n",
    "    print(f\"   Episodes: {len(df_td3)}\")\n",
    "    print(f\"   Colonnes: {list(df_td3.columns)}\")\n",
    "else:\n",
    "    print(\" Aucun log TD3 trouvÃ©\")\n",
    "    df_td3 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621b0dd",
   "metadata": {},
   "source": [
    "## 2. AperÃ§u des DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les premiÃ¨res lignes SAC\n",
    "if df_sac is not None:\n",
    "    print(\" SAC - PremiÃ¨res lignes:\")\n",
    "    display(df_sac.head(10))\n",
    "    \n",
    "    print(\"\\n Statistiques SAC:\")\n",
    "    display(df_sac.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les premiÃ¨res lignes TD3\n",
    "if df_td3 is not None:\n",
    "    print(\"ğŸ“‹ TD3 - PremiÃ¨res lignes:\")\n",
    "    display(df_td3.head(10))\n",
    "    \n",
    "    print(\"\\nğŸ“Š Statistiques TD3:\")\n",
    "    display(df_td3.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49647d3a",
   "metadata": {},
   "source": [
    "## 3. Visualisation des Losses - SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2951e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_sac is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('SAC - Evolution des Losses', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Critic Loss\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(df_sac['Episode'], df_sac['CriticLoss'], label='Critic Loss', color='blue', alpha=0.6)\n",
    "    ax1.plot(df_sac['Episode'], df_sac['CriticLoss'].rolling(50).mean(), \n",
    "             label='Moving Average (50)', color='darkblue', linewidth=2)\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Critic Loss')\n",
    "    ax1.set_title('Critic Loss (Q-networks)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Actor Loss\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(df_sac['Episode'], df_sac['ActorLoss'], label='Actor Loss', color='green', alpha=0.6)\n",
    "    ax2.plot(df_sac['Episode'], df_sac['ActorLoss'].rolling(50).mean(), \n",
    "             label='Moving Average (50)', color='darkgreen', linewidth=2)\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Actor Loss')\n",
    "    ax2.set_title('Actor Loss (Policy)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Alpha Loss (temperature tuning)\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(df_sac['Episode'], df_sac['AlphaLoss'], label='Alpha Loss', color='red', alpha=0.6)\n",
    "    ax3.plot(df_sac['Episode'], df_sac['AlphaLoss'].rolling(50).mean(), \n",
    "             label='Moving Average (50)', color='darkred', linewidth=2)\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Alpha Loss')\n",
    "    ax3.set_title('Alpha Loss (Entropy Temperature)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Alpha Value\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.plot(df_sac['Episode'], df_sac['Alpha'], label='Alpha', color='purple', linewidth=2)\n",
    "    ax4.set_xlabel('Episode')\n",
    "    ax4.set_ylabel('Alpha Value')\n",
    "    ax4.set_title('Alpha Value (Auto-tuned Temperature)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/sac_losses.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Graphique SAC sauvegardÃ©: results/plots/sac_losses.png\")\n",
    "else:\n",
    "    print(\"âš ï¸ Pas de donnÃ©es SAC Ã  visualiser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143f02f",
   "metadata": {},
   "source": [
    "## 4. Visualisation des Losses - TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1291f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_td3 is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    fig.suptitle('TD3 - Evolution des Losses', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Critic Loss\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(df_td3['Episode'], df_td3['CriticLoss'], label='Critic Loss', color='blue', alpha=0.6)\n",
    "    ax1.plot(df_td3['Episode'], df_td3['CriticLoss'].rolling(50).mean(), \n",
    "             label='Moving Average (50)', color='darkblue', linewidth=2)\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Critic Loss')\n",
    "    ax1.set_title('Critic Loss (Twin Q-networks)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Actor Loss\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(df_td3['Episode'], df_td3['ActorLoss'], label='Actor Loss', color='green', alpha=0.6)\n",
    "    ax2.plot(df_td3['Episode'], df_td3['ActorLoss'].rolling(50).mean(), \n",
    "             label='Moving Average (50)', color='darkgreen', linewidth=2)\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Actor Loss')\n",
    "    ax2.set_title('Actor Loss (Deterministic Policy)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/td3_losses.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Graphique TD3 sauvegardÃ©: results/plots/td3_losses.png\")\n",
    "else:\n",
    "    print(\"âš ï¸ Pas de donnÃ©es TD3 Ã  visualiser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0558d0",
   "metadata": {},
   "source": [
    "## 5. Comparaison SAC vs TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb015b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_sac is not None and df_td3 is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('SAC vs TD3 - Comparaison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Critic Loss Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(df_sac['Episode'], df_sac['CriticLoss'].rolling(50).mean(), \n",
    "             label='SAC', color='blue', linewidth=2)\n",
    "    ax1.plot(df_td3['Episode'], df_td3['CriticLoss'].rolling(50).mean(), \n",
    "             label='TD3', color='red', linewidth=2)\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Critic Loss (MA-50)')\n",
    "    ax1.set_title('Critic Loss Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Actor Loss Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(df_sac['Episode'], df_sac['ActorLoss'].rolling(50).mean(), \n",
    "             label='SAC', color='blue', linewidth=2)\n",
    "    ax2.plot(df_td3['Episode'], df_td3['ActorLoss'].rolling(50).mean(), \n",
    "             label='TD3', color='red', linewidth=2)\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Actor Loss (MA-50)')\n",
    "    ax2.set_title('Actor Loss Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reward Comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(df_sac['Episode'], df_sac['Reward'].rolling(50).mean(), \n",
    "             label='SAC', color='blue', linewidth=2)\n",
    "    ax3.plot(df_td3['Episode'], df_td3['Reward'].rolling(50).mean(), \n",
    "             label='TD3', color='red', linewidth=2)\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Reward (MA-50)')\n",
    "    ax3.set_title('Training Reward Comparison')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Eval Reward Comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    sac_eval = df_sac[df_sac['EvalReward'].notna()]\n",
    "    td3_eval = df_td3[df_td3['EvalReward'].notna()]\n",
    "    ax4.plot(sac_eval['Episode'], sac_eval['EvalReward'], \n",
    "             label='SAC', color='blue', marker='o', linewidth=2)\n",
    "    ax4.plot(td3_eval['Episode'], td3_eval['EvalReward'], \n",
    "             label='TD3', color='red', marker='s', linewidth=2)\n",
    "    ax4.set_xlabel('Episode')\n",
    "    ax4.set_ylabel('Evaluation Reward')\n",
    "    ax4.set_title('Evaluation Reward Comparison')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/sac_vs_td3_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Comparaison sauvegardÃ©e: results/plots/sac_vs_td3_comparison.png\")\n",
    "else:\n",
    "    print(\"âš ï¸ Il faut les deux algorithmes pour faire une comparaison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b2b2f",
   "metadata": {},
   "source": [
    "## 6. Statistiques DÃ©taillÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e369c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer les statistiques\n",
    "def compute_stats(df, algo_name):\n",
    "    \"\"\"Calcule statistiques d'entraÃ®nement\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š STATISTIQUES - {algo_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Episodes\n",
    "    print(f\"\\nğŸ“ˆ Episodes:\")\n",
    "    print(f\"   Total: {len(df)}\")\n",
    "    print(f\"   DÃ©but: {df['Episode'].min()}\")\n",
    "    print(f\"   Fin: {df['Episode'].max()}\")\n",
    "    \n",
    "    # Losses (derniers 100 episodes)\n",
    "    last_100 = df.tail(100)\n",
    "    print(f\"\\nğŸ¯ Losses (derniers 100 episodes):\")\n",
    "    print(f\"   Critic Loss: {last_100['CriticLoss'].mean():.6f} Â± {last_100['CriticLoss'].std():.6f}\")\n",
    "    print(f\"   Actor Loss: {last_100['ActorLoss'].mean():.6f} Â± {last_100['ActorLoss'].std():.6f}\")\n",
    "    \n",
    "    if 'AlphaLoss' in df.columns:\n",
    "        print(f\"   Alpha Loss: {last_100['AlphaLoss'].mean():.6f} Â± {last_100['AlphaLoss'].std():.6f}\")\n",
    "        print(f\"   Alpha Value: {last_100['Alpha'].mean():.4f} Â± {last_100['Alpha'].std():.4f}\")\n",
    "    \n",
    "    # Rewards\n",
    "    print(f\"\\nğŸ† Rewards:\")\n",
    "    print(f\"   Training - Moyenne: {df['Reward'].mean():.2f}\")\n",
    "    print(f\"   Training - Meilleur: {df['Reward'].max():.2f}\")\n",
    "    print(f\"   Training - Dernier 100: {last_100['Reward'].mean():.2f} Â± {last_100['Reward'].std():.2f}\")\n",
    "    \n",
    "    if 'EvalReward' in df.columns:\n",
    "        eval_df = df[df['EvalReward'].notna()]\n",
    "        if len(eval_df) > 0:\n",
    "            print(f\"   Eval - Moyenne: {eval_df['EvalReward'].mean():.2f}\")\n",
    "            print(f\"   Eval - Meilleur: {eval_df['EvalReward'].max():.2f}\")\n",
    "    \n",
    "    # Survie\n",
    "    print(f\"\\nâ±ï¸ Survie:\")\n",
    "    print(f\"   Moyenne: {df['Survival'].mean():.1f} timesteps\")\n",
    "    print(f\"   Maximum: {df['Survival'].max():.1f} timesteps\")\n",
    "    print(f\"   Dernier 100: {last_100['Survival'].mean():.1f} Â± {last_100['Survival'].std():.1f}\")\n",
    "    \n",
    "    # Buffer\n",
    "    if 'BufferSize' in df.columns:\n",
    "        print(f\"\\nğŸ’¾ Replay Buffer:\")\n",
    "        print(f\"   Taille finale: {df['BufferSize'].iloc[-1]:,} transitions\")\n",
    "\n",
    "# Afficher stats pour chaque algorithme\n",
    "if df_sac is not None:\n",
    "    compute_stats(df_sac, \"SAC\")\n",
    "\n",
    "if df_td3 is not None:\n",
    "    compute_stats(df_td3, \"TD3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcdde54",
   "metadata": {},
   "source": [
    "## 7. Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05993d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_convergence(df, algo_name, reward_threshold=-50):\n",
    "    \"\"\"Analyse si l'algorithme a convergÃ©\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ¯ ANALYSE DE CONVERGENCE - {algo_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Calculer moving average sur 100 episodes\n",
    "    df['Reward_MA100'] = df['Reward'].rolling(100).mean()\n",
    "    \n",
    "    # Trouver quand la moyenne atteint le seuil\n",
    "    converged = df[df['Reward_MA100'] >= reward_threshold]\n",
    "    \n",
    "    if len(converged) > 0:\n",
    "        convergence_episode = converged.iloc[0]['Episode']\n",
    "        print(f\"âœ… Convergence atteinte Ã  l'Ã©pisode {convergence_episode}\")\n",
    "        print(f\"   Seuil: {reward_threshold}\")\n",
    "        print(f\"   Reward final (MA-100): {df['Reward_MA100'].iloc[-1]:.2f}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Pas encore convergÃ© (seuil: {reward_threshold})\")\n",
    "        print(f\"   Meilleur reward (MA-100): {df['Reward_MA100'].max():.2f}\")\n",
    "    \n",
    "    # StabilitÃ© (variance des derniers 20% episodes)\n",
    "    last_20pct = df.tail(int(len(df) * 0.2))\n",
    "    stability = last_20pct['Reward'].std()\n",
    "    print(f\"\\nğŸ“Š StabilitÃ© (std derniers 20%): {stability:.2f}\")\n",
    "    if stability < 50:\n",
    "        print(\"   âœ… TrÃ¨s stable\")\n",
    "    elif stability < 100:\n",
    "        print(\"   âš ï¸ ModÃ©rÃ©ment stable\")\n",
    "    else:\n",
    "        print(\"   âŒ Instable\")\n",
    "\n",
    "if df_sac is not None:\n",
    "    analyze_convergence(df_sac, \"SAC\")\n",
    "\n",
    "if df_td3 is not None:\n",
    "    analyze_convergence(df_td3, \"TD3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2431a",
   "metadata": {},
   "source": [
    "## 8. Export des Graphiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ff7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er le dossier plots s'il n'existe pas\n",
    "plots_dir = Path('../results/plots')\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Tous les graphiques ont Ã©tÃ© sauvegardÃ©s dans:\")\n",
    "print(f\"   {plots_dir.absolute()}\")\n",
    "print(\"\\nFichiers crÃ©Ã©s:\")\n",
    "for plot_file in plots_dir.glob('*.png'):\n",
    "    print(f\"   - {plot_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5092ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ Notes\n",
    "\n",
    "**InterprÃ©tation des losses:**\n",
    "\n",
    "1. **Critic Loss** : Doit diminuer progressivement â†’ Le rÃ©seau apprend Ã  estimer la valeur Q\n",
    "2. **Actor Loss** : Peut Ãªtre nÃ©gatif (normal!) â†’ On maximise la Q-value prÃ©dite\n",
    "3. **Alpha Loss** (SAC) : S'auto-ajuste â†’ Ã‰quilibre exploration/exploitation\n",
    "4. **Alpha Value** (SAC) : \n",
    "   - Ã‰levÃ© (>0.5) â†’ Plus d'exploration (stochastique)\n",
    "   - Faible (<0.1) â†’ Plus d'exploitation (dÃ©terministe)\n",
    "\n",
    "**Signes de bon entraÃ®nement:**\n",
    "- âœ… Losses qui diminuent progressivement\n",
    "- âœ… Reward qui augmente\n",
    "- âœ… Faible variance dans les derniers Ã©pisodes\n",
    "- âœ… Eval reward > Training reward (gÃ©nÃ©ralisation)\n",
    "\n",
    "**Signes de problÃ¨mes:**\n",
    "- âŒ Losses qui explosent (NaN/Inf) â†’ Learning rate trop Ã©levÃ©\n",
    "- âŒ Losses qui stagnent â†’ RÃ©seau bloquÃ©, essayer autre seed\n",
    "- âŒ Reward qui oscille â†’ Pas assez stable, augmenter batch size\n",
    "- âŒ Eval reward << Training reward â†’ Overfitting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
